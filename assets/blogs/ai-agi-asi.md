**AI** is all the hype. Researchers talk about “solving AI", but engineers worry about “AI replacing jobs.” People fear “AI will trick us and destroy humanity”, yet the same “AI” can fumble counting the _r_’s in “strawberry,” and robots that are supposedly “AI-powered” struggle with everyday tasks that babies do effortlessly.

How come this contrast?

Those statements _sound_ contradictory, but they are all true simultaneously because the word **AI** quietly changes meaning depending on who’s using it. Sometimes its actually referring to **Artificial Intelligence**, other times its either referring to either **Artificial General Intelligence** or **Artificial Superintelligence**.

**Artificial Intelligence (AI)** currently means large language models (like ChatGPT or Gemini) or image/video generators (diffusion models). These systems can produce impressively “smart” outputs: they can write, summarize, code, brainstorm, and generate images. But they cannot reliably _act_ in the world on their own. They can help you plan a recipe, but they can’t walk into your kitchen and cook it. They can describe how to draw with pastel colors, but they can’t pick up a pastel and do it. They’re powerful tools but not general-purpose agents.

**Artificial General Intelligence (AGI)** is the thing most researchers mean when they talk about “solving AI.” It’s a system with human-level generalization ability. It can learn new tasks, reason across domains, and operate in the real world without being specifically trained for every single specific task. That “general” part is the key. AGI wouldn’t just be good at _one_ narrow capability (text or images), it would perceive, reason, remember and act in the world the way humans do. This is the goal. Solving this will finally enable robots that can sweep your house and make you tea every morning.

**Artificial Superintelligence (ASI)** is the scary version people jump to without realizing they’ve changed terms mid sentence. ASI refers to an AGI system that is supremely more intelligent as compared to us mere mortal humans to the point it easily manipulates us. Much like we train dogs to do things by tricking them with treats, ASI might trick humans into acting in ways that eventually would provide it all the control.  The classic fear is not “a robot with strong arms,” but a mind that’s _so_ far ahead that our attempts to control it look like a dog trying to negotiate with its owner. This is what we fear would destroy humanity.


Now the first paragraph becomes much clearer if we swap in the intended meanings:

**AI** is currently all the hype. Researchers are trying to solve **AGI**. People fear **AGI** taking over their jobs. A significant number of people worry **ASI** will trick and destroy humanity.

Same paragraph. Much less confusion.